1. Create project template by executing template.py file
2. Write the code on setup.py and pyproject.toml file to import local packages
   >> Find more about "setup.py and pyproject.toml" at crashcourse.txt
3. Create a virtual env, activate it and install the requirements from requirements.txt
   conda create -n vehicle python=3.10 -y
   conda activate vehicle
   add required modules to requirements.txt
   Do "pip install -r requirements.txt"
4. Do a "pip list" on terminal to make sure you have local packages installed.

----------------------------------------------- MongoDB Setup -----------------------------------------------
5. Sign up to MongoDB Atlas and create a new project by just providing it a name then next next create.
6. From "Create a cluster" screen, hit "create", Select M0 service keeping other services as default, hit "create deployment"
7. Setup the username and password and then create DB user.
8. Go to "network access" and add ip address - "0.0.0.0/0" so that we can access it from anywhere
9. Go back to project >> "Get Connection String" >> "Drivers" >> {Driver:Python, Version:3.6 or later} 
   >> copy and save the connection string with you(replace password). >> Done.
10. Create folder "notebook" >> do step 11 >>  create file "mongoDB_demo.ipynb" >> select kernal>python kernal>vehicle>>
11. Dataset added to notebook folder
12. Push your data to mongoDB database from your python notebook.
13. Go to mongoDB Atlas >> Database >> browse collection >> see your data in key value format 

-------------------------------------- logging, exception and notebooks --------------------------------------
14. Write the logger file and test it on demo.py
15. Write the exception file and test it on demo.py
16. EDA and Feature Engg notebook added.

----------------------------------------------- Data Ingestion -----------------------------------------------
17. Before we work on "Data Ingestion" component >> Declare variables within constants.__init__.py file >> 
    add code to configuration.mongo_db_connections.py file and define the func for mondodb connection >> 
    Inside "data_access" folder, add code to proj1_data that will use mongo_db_connections.py
    to connect with DB, fetch data in key-val format and transform that to df >>
    add code to entity.config_entity.py file till DataIngestionConfig class >>
    add code to entity.artifact_entity.py file till DataIngestionArtifact class >>
    add code to components.data_ingestion.py file >> add code to training pipeline >> 
    run demo.py (set mongodb connection url first, see next step)
18. To setup the connection url on mac(also work for windows), open bash/powershell terminal and run below command:
                        *** For Bash ***
    set: export MONGODB_URL="mongodb+srv://<username>:<password>......"
    check: echo $MONGODB_URL
                        *** For Powershell ***
    set: $env:MONGODB_URL = "mongodb+srv://<username>:<password>......"
    check: echo $env:MONGODB_URL

    To setup the connection url on Windows, open env variable setting option and add a new variable:
    Name: MONGODB_URL, Value = <url>
    Also add "artifact" dir to .gitignore file







# promethues is used to monitoring: by default promethus monitor its own and even the new entities it has to monitor can be added

1.docker run -d --name prometheus-container -p 9090:9090 prom/prometheus:latest  --> to install promethus
https://psychic-space-spork-5rw6w5xxrgwf44g9-9090.app.github.dev/metrics
 
# to run docker container to get the node exporter for linux
2.docker run -d \
  --name=node-exporter \
  --restart=unless-stopped \
  -p 9100:9100 \
  -v "/proc:/host/proc:ro" \
  -v "/sys:/host/sys:ro" \
  -v "/:/rootfs:ro" \
  prom/node-exporter:latest \
  --path.procfs=/host/proc \
  --path.sysfs=/host/sys \
  --path.rootfs=/rootfs

3.docker exec -it prometheus-container cat /etc/prometheus/prometheus.yml -> to see the prometheus.yml file in the running docker container

4.docker cp <container_id>:/etc/prometheus/prometheus.yml ./prometheus.yml -> copy the yaml from container to local 
docker cp 0e292c551053:/etc/prometheus/prometheus.yml ./prometheus.yml

5.docker cp ./prometheus.yml <container_id>:/etc/prometheus/prometheus.yml -> copy the edited prometheus.yml file from local to running prometheus container
docker cp ./prometheus.yml 2306cca774d8:/etc/prometheus/prometheus.yml # in the edited prometheus file we are telling the prometheus to motinor even the metrics on port 9100, where node exporter metrics is present 

6.docker restart <container_id> -> to restart the container after updating the '/etc/prometheus/prometheus.yml' file

now need to see both promethus and node jobs in Target health in promethus

#------------------------------------------------------------------------------
docker run -d -p 3000:3000 --name=grafana grafana/grafana-enterprise:latest -> to start the grafana service
admin/admin
Under data source provide --> the IP of the promethus container ( grafana is used to visulise the monitoring data in promethus and create alerts )
In grafana can connect to multiple source( including promethus ) to get the data from source and can build different types of visualisations and dashboards and create alerts triggers to emails



